{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb375c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/home/fdivaler/CW\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4ca4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/fdivaler/miniconda3/envs/graphcw/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': {'gnn_saving_dir': '', 'gnn_name': 'gat', 'n_heads': 3, 'param': {'sider': {'learning_rate': 0.001, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 64, 'num_epochs': 200, 'num_early_stop': 20, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'hiv': {'learning_rate': 0.001, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 64, 'num_epochs': 200, 'num_early_stop': 20, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'sum', 'fc_latent_dim': [128], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'clintox': {'learning_rate': 0.0005, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 64, 'num_epochs': 200, 'num_early_stop': 0, 'gnn_latent_dim': [64, 64, 64], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'sigmoid', 'readout': 'sum', 'fc_latent_dim': [128], 'fc_dropout': 0.0, 'fc_nonlinear': 'elu'}, 'tox21': {'learning_rate': 0.001, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 64, 'num_epochs': 200, 'num_early_stop': 20, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'ba_lrp': {'learning_rate': 0.001, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 32, 'num_epochs': 200, 'num_early_stop': 0, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'bbbp': {'learning_rate': 0.001, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 64, 'num_epochs': 200, 'num_early_stop': 20, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'sum', 'fc_latent_dim': [128], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'graph_sst2': {'learning_rate': 0.001, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 128, 'num_epochs': 50, 'num_early_stop': 0, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'graph_sst5': {'learning_rate': 0.001, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 128, 'num_epochs': 50, 'num_early_stop': 0, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'twitter': {'learning_rate': 0.001, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 128, 'num_epochs': 50, 'num_early_stop': 0, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': True, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'ba_shapes': {'learning_rate': 0.05, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 1, 'num_epochs': 400, 'num_early_stop': 0, 'gnn_latent_dim': [20, 20, 20], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': False, 'graph_classification': False, 'node_classification': True, 'gnn_nonlinear': 'relu', 'readout': 'identity', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'ba_community': {'learning_rate': 0.01, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 1, 'num_epochs': 800, 'num_early_stop': 0, 'gnn_latent_dim': [20, 20, 20], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': True, 'graph_classification': False, 'node_classification': True, 'gnn_nonlinear': 'relu', 'readout': 'identity', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'tree_grid': {'learning_rate': 0.01, 'weight_decay': 0.0005, 'milestones': 'None', 'gamma': 'None', 'batch_size': 1, 'num_epochs': 800, 'num_early_stop': 0, 'gnn_latent_dim': [40, 40, 40], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': True, 'graph_classification': False, 'node_classification': True, 'gnn_nonlinear': 'relu', 'readout': 'identity', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'tree_cycle': {'learning_rate': 0.01, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 1, 'num_epochs': 800, 'num_early_stop': 0, 'gnn_latent_dim': [20, 20, 20], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': True, 'graph_classification': False, 'node_classification': True, 'gnn_nonlinear': 'relu', 'readout': 'identity', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'ba_2motifs': {'learning_rate': 0.01, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 32, 'num_epochs': 300, 'num_early_stop': 0, 'gnn_latent_dim': [20, 20, 20], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'mean', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}, 'bace': {'learning_rate': 0.01, 'weight_decay': 0.0, 'milestones': 'None', 'gamma': 'None', 'batch_size': 128, 'num_epochs': 400, 'num_early_stop': 20, 'gnn_latent_dim': [128, 128, 128], 'gnn_dropout': 0.0, 'add_self_loop': True, 'gcn_adj_normalization': False, 'gnn_emb_normalization': False, 'graph_classification': True, 'node_classification': False, 'gnn_nonlinear': 'relu', 'readout': 'max', 'fc_latent_dim': [], 'fc_dropout': 0.0, 'fc_nonlinear': 'relu'}}}, 'datasets': {'dataset_root': '/tmp/datasets', 'dataset_name': 'bbbp', 'random_split_flag': True, 'data_split_ratio': [0.8, 0.1, 0.1], 'seed': 123}, 'explainers': {'explanation_result_dir': '', 'sparsity': 0.5, 'param': {'bbbp': {'none': 'None'}, 'graph_sst2': {'none': 'None'}, 'graph_sst5': {'none': 'None'}, 'twitter': {'none': 'None'}, 'ba_shapes': {'none': 'None'}, 'ba_community': {'none': 'None'}, 'tree_grid': {'none': 'None'}, 'tree_cycle': {'none': 'None'}, 'ba_2motifs': {'none': 'None'}, 'bace': {'none': 'None'}}}, 'device_id': 2, 'model_name': 'GCN_3l', 'record_filename': 'none', 'base_dir': '/raid/home/fdivaler/CW/GraphCW/', 'statistics_dir': '/raid/home/fdivaler/CW/GraphCW/trainings/', 'statistics_file': 'gat_cw_max.json', 'stratified': False, 'train_flag': True, 'concept_whitening': False, 'concept_dir': '/raid/home/fdivaler/CW/GraphCW/molecular_concepts/', 'concepts': 'qed,logp,tpsa,NOCount,n_heteroatoms'}\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# Initialize Hydra with the config folder\n",
    "# GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"config\", job_name=\"nb\")\n",
    "\n",
    "# Load the config file\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Now cfg is your OmegaConf DictConfig object\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4bf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dig.xgraph.dataset import MoleculeDataset, SynGraphDataset, SentiGraphDataset, BA_LRP\n",
    "from dataset import get_dataset\n",
    "d = get_dataset(dataset_root=cfg.datasets.dataset_root,\n",
    "                          dataset_name=cfg.datasets.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d4c44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='[Cl].CC(C)NCC(O)COc1cccc2ccccc12')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf5dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_custom.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = [1, 6, 7, 8, 9, 15, 16, 17, 35, 53]  # H, C, N, O, F, P, S, Cl, Br, I\n",
    "EDGE_FEAT_DIM = 9  # SINGLE, DOUBLE, TRIPLE, AROMATIC, conj, ring, stereo-none, stereo-Z, stereo-E\n",
    "\n",
    "\n",
    "def one_hot(x, xs):\n",
    "    return [1.0 if x == s else 0.0 for s in xs]\n",
    "\n",
    "# def atom_features(atom):\n",
    "#     Z = atom.GetAtomicNum()\n",
    "#     atom_onehot = [1.0 if Z == z else 0.0 for z in ATOM_LIST] + [0.0 if Z in ATOM_LIST else 1.0]\n",
    "#     degree      = one_hot(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5])\n",
    "#     num_hs      = one_hot(atom.GetTotalNumHs(), [0, 1, 2, 3, 4])\n",
    "#     aromatic    = [1.0 if atom.GetIsAromatic() else 0.0]\n",
    "#     charge      = [float(atom.GetFormalCharge())]\n",
    "#     return atom_onehot + degree + num_hs + aromatic + charge  # -> float features\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# ---- atoms ----\n",
    "HYB_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2\n",
    "]\n",
    "CHIR_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW\n",
    "]\n",
    "\n",
    "def clip_one_hot(val, bins):\n",
    "    val = val if val in bins else max(bins)   # bucket “>=max”\n",
    "    return [1.0 if val == b else 0.0 for b in bins]\n",
    "\n",
    "def atom_features(a: Chem.Atom):\n",
    "    Z = a.GetAtomicNum()\n",
    "    atom_type = [1.0 if Z == z else 0.0 for z in ATOM_LIST] + [0.0 if Z in ATOM_LIST else 1.0]\n",
    "    degree    = clip_one_hot(a.GetTotalDegree(), [0,1,2,3,4,5])\n",
    "    num_hs    = clip_one_hot(a.GetTotalNumHs(), [0,1,2,3,4])\n",
    "    aromatic  = [1.0 if a.GetIsAromatic() else 0.0]\n",
    "    formal    = [float(a.GetFormalCharge())]\n",
    "    in_ring   = [1.0 if a.IsInRing() else 0.0]\n",
    "    hybrid    = [1.0 if a.GetHybridization()==h else 0.0 for h in HYB_LIST]\n",
    "    chir      = [1.0 if a.GetChiralTag()==c else 0.0 for c in CHIR_LIST]\n",
    "    mass      = [a.GetMass()/100.0]  # mild scale\n",
    "\n",
    "    return atom_type + degree + num_hs + aromatic + formal + in_ring + hybrid + chir + mass\n",
    "\n",
    "def _largest_fragment(mol: Chem.Mol) -> Chem.Mol:\n",
    "    \"\"\"Keep the largest fragment (handles salts / multi-fragment SMILES).\"\"\"\n",
    "    frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "    return max(frags, key=lambda m: m.GetNumAtoms()) if len(frags) > 1 else mol\n",
    "\n",
    "# ---- bonds (edge_attr) ----\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "    Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "\n",
    "def bond_features(b: Chem.Bond):\n",
    "    btype = [1.0 if b.GetBondType()==t else 0.0 for t in BOND_LIST]\n",
    "    conj  = [1.0 if b.GetIsConjugated() else 0.0]\n",
    "    ring  = [1.0 if b.IsInRing() else 0.0]\n",
    "    stereo = [\n",
    "        1.0 if b.GetStereo()==Chem.rdchem.BondStereo.STEREONONE else 0.0,\n",
    "        1.0 if b.GetStereo()==Chem.rdchem.BondStereo.STEREOZ else 0.0,\n",
    "        1.0 if b.GetStereo()==Chem.rdchem.BondStereo.STEREOE else 0.0,\n",
    "    ]\n",
    "    return btype + conj + ring + stereo\n",
    "\n",
    "\n",
    "class ChEMBLActivityDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Expects a CSV at:   {root}/chembl_ic50/raw/data.csv\n",
    "    with at least columns: 'ISOMERIC SMILES' and 'pAct'\n",
    "    Labels are binary: y = 1 if pAct >= threshold else 0\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        csv_filename='data.csv',\n",
    "        id_col = 'ID',\n",
    "        smiles_col='ISOMERIC SMILES',\n",
    "        activity_col='pAct',\n",
    "        threshold=6.0,                   # pAct >= 6 is a common activity cutoff\n",
    "        transform=None, pre_transform=None, pre_filter=None\n",
    "    ):\n",
    "        self.csv_filename = csv_filename\n",
    "        self.id_col = id_col\n",
    "        self.smiles_col   = smiles_col\n",
    "        self.activity_col = activity_col\n",
    "        self.threshold    = float(threshold)\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    # ----- properties expected by your code -----\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'chembl_ic50'\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "\n",
    "    # ----- required InMemoryDataset API -----\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.csv_filename]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # def process(self):\n",
    "    #     df = pd.read_csv(self.raw_paths[0])\n",
    "    #     data_list = []\n",
    "\n",
    "    #     for _, row in df.iterrows():\n",
    "    #         smi = str(row[self.smiles_col])\n",
    "    #         mol = Chem.MolFromSmiles(smi)\n",
    "    #         if mol is None:\n",
    "    #             continue\n",
    "\n",
    "    #         # label\n",
    "    #         id = str(row[self.id_col])\n",
    "    #         pact = float(row[self.activity_col])\n",
    "    #         y = torch.tensor([1 if pact >= self.threshold else 0], dtype=torch.long)\n",
    "\n",
    "    #         # nodes\n",
    "    #         x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)\n",
    "    #         # edges (undirected)\n",
    "    #         edge_index = []\n",
    "    #         for b in mol.GetBonds():\n",
    "    #             i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "    #             edge_index.append([i, j]); edge_index.append([j, i])\n",
    "    #         edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() if len(edge_index) \\\n",
    "    #                      else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    #         data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    #         data_list.append(data)\n",
    "\n",
    "    #     if self.pre_filter is not None:\n",
    "    #         data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "    #     if self.pre_transform is not None:\n",
    "    #         data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "    #     data, slices = self.collate(data_list)\n",
    "    #     torch.save((data, slices), self.processed_paths[0])\n",
    "    def process(self):\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "\n",
    "        data_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            smi = str(row[self.smiles_col])\n",
    "            pact_val = row[self.activity_col]\n",
    "\n",
    "            # label: binary from pAct threshold\n",
    "            try:\n",
    "                pact = float(pact_val)\n",
    "            except Exception:\n",
    "                continue\n",
    "            y = torch.tensor([1 if pact >= self.threshold else 0], dtype=torch.long)\n",
    "\n",
    "            # robust SMILES -> RDKit mol\n",
    "            mol = Chem.MolFromSmiles(smi, sanitize=True)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            if '.' in smi:\n",
    "                mol = _largest_fragment(mol)\n",
    "\n",
    "            # node features\n",
    "            x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)\n",
    "\n",
    "            # edges + edge_attr (undirected)\n",
    "            ei, ea = [], []\n",
    "            for b in mol.GetBonds():\n",
    "                i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "                f = bond_features(b)\n",
    "                ei += [[i, j], [j, i]]\n",
    "                ea += [f, f]\n",
    "\n",
    "            if ei:\n",
    "                edge_index = torch.tensor(ei, dtype=torch.long).t().contiguous()\n",
    "                edge_attr  = torch.tensor(ea, dtype=torch.float)\n",
    "            else:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "                edge_attr  = torch.empty((0, EDGE_FEAT_DIM), dtype=torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y, edge_attr=edge_attr)  # <-- always set\n",
    "\n",
    "            # if edge_attr is not None:\n",
    "            #     data.edge_attr = edge_attr\n",
    "\n",
    "            # optional filtering / transforms\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab07703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prerequisites ---\n",
    "# pip install rdkit-pypi torch-geometric pandas scikit-learn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dig.xgraph.dataset import MoleculeDataset, SynGraphDataset, SentiGraphDataset, BA_LRP\n",
    "from torch import default_generator\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(dataset_root, dataset_name, pre_filter=None, threshold=6.0):\n",
    "    if dataset_name.lower() in list(MoleculeDataset.names.keys()):\n",
    "        return MoleculeDataset(root=dataset_root, name=dataset_name, pre_filter=pre_filter)\n",
    "    elif dataset_name.lower() in ['graph_sst2', 'graph_sst5', 'twitter']:\n",
    "        return SentiGraphDataset(root=dataset_root, name=dataset_name)\n",
    "    elif dataset_name.lower() in list(SynGraphDataset.names.keys()):\n",
    "        return SynGraphDataset(root=dataset_root, name=dataset_name)\n",
    "    elif dataset_name.lower() in ['ba_lrp']:\n",
    "        return BA_LRP(root=dataset_root)\n",
    "    # >>> add this:\n",
    "    elif dataset_name.lower() in ['chembl_ic50']:\n",
    "        return ChEMBLActivityDataset(root=os.path.join(dataset_root, 'chembl_ic50'),threshold=threshold)\n",
    "    else:\n",
    "        raise ValueError(f\"{dataset_name} is not defined.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, stratified, random_split_flag=True, data_split_ratio=None, seed=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset:\n",
    "        batch_size: int\n",
    "        random_split_flag: bool\n",
    "        data_split_ratio: list, training, validation and testing ratio\n",
    "        seed: random seed to split the dataset randomly\n",
    "    Returns:\n",
    "        a dictionary of training, validation, and testing dataLoader\n",
    "    \"\"\"\n",
    "    dataloader = dict()\n",
    "    \n",
    "    if not stratified:\n",
    "        if not random_split_flag and hasattr(dataset, 'supplement'):\n",
    "            assert 'split_indices' in dataset.supplement.keys(), \"split idx\"\n",
    "            split_indices = dataset.supplement['split_indices']\n",
    "            train_indices = torch.where(split_indices == 0)[0].numpy().tolist()\n",
    "            dev_indices = torch.where(split_indices == 1)[0].numpy().tolist()\n",
    "            test_indices = torch.where(split_indices == 2)[0].numpy().tolist()\n",
    "\n",
    "            train = Subset(dataset, train_indices)\n",
    "            eval = Subset(dataset, dev_indices)\n",
    "            test = Subset(dataset, test_indices)\n",
    "        else:\n",
    "            num_train = int(data_split_ratio[0] * len(dataset))\n",
    "            num_eval = int(data_split_ratio[1] * len(dataset))\n",
    "            num_test = len(dataset) - num_train - num_eval\n",
    "\n",
    "            train, eval, test = random_split(dataset,\n",
    "                                             lengths=[num_train, num_eval, num_test],\n",
    "                                             generator=default_generator)\n",
    "            \n",
    "        dataloader['train'] = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        dataloader['eval'] = DataLoader(eval, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        dataloader['test'] = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        targets = []\n",
    "        for sample in dataset:\n",
    "            targets.append(torch.argmax(sample.y).item())\n",
    "\n",
    "        train_idx, test_idx = train_test_split(range(len(targets)),\n",
    "                                                test_size=(1-data_split_ratio[0]),\n",
    "                                                random_state=seed,\n",
    "                                                shuffle=True,\n",
    "                                                stratify=targets)\n",
    "\n",
    "        test_targets = []\n",
    "        for idx in test_idx:\n",
    "            test_targets.append(targets[idx])\n",
    "\n",
    "        if data_split_ratio[1] == data_split_ratio[2]:\n",
    "\n",
    "            valid_idx, test_idx = train_test_split(range(len(test_targets)),\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=seed,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=test_targets)\n",
    "\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "\n",
    "        dataloader = dict()\n",
    "        dataloader['train'] = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "        dataloader['test'] = DataLoader(dataset, batch_size=1, sampler=test_sampler, drop_last=True)\n",
    "\n",
    "        if data_split_ratio[1] == data_split_ratio[2]:\n",
    "            valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "            dataloader['eval'] = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, drop_last=True)\n",
    "    \n",
    "    return dataloader\n",
    "# (If you want to pass a custom pAct threshold, instantiate dataset_custom.ChEMBLActivityDataset directly.)\n",
    "\n",
    "def build_chembl_dataset_from_df(df: pd.DataFrame, dataset_root: str, threshold=6.0) -> object:\n",
    "    \"\"\"\n",
    "    Saves `df` to the path expected by ChEMBLActivityDataset and returns the dataset.\n",
    "    df must contain at least: 'ISOMERIC SMILES' and 'pAct'.\n",
    "    \"\"\"\n",
    "    # keep only the columns the dataset needs\n",
    "    req_cols = ['ID','ISOMERIC SMILES','MW','pAct']\n",
    "    missing = [c for c in req_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df2 = df[req_cols].copy()\n",
    "    # basic cleaning\n",
    "    df2 = df2.dropna(subset=req_cols)\n",
    "    df2['pAct'] = pd.to_numeric(df2['pAct'], errors='coerce')\n",
    "    df2 = df2.dropna(subset=['pAct'])\n",
    "\n",
    "    # write CSV to the expected raw location: <root>/chembl_ic50/raw/data.csv\n",
    "    raw_dir = os.path.join(dataset_root, 'chembl_ic50', 'raw')\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(raw_dir, 'data.csv')\n",
    "    df2.to_csv(csv_path, index=False)\n",
    "\n",
    "    # build the dataset via your get_dataset() router\n",
    "    dataset = get_dataset(dataset_root=dataset_root, dataset_name='chembl_ic50', threshold=threshold)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf91bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "imported = pickle.load(open('./transferCW/Selezione_dati_attivita.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bffd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5079, dtype=torch.float64) +- tensor(1.2877, dtype=torch.float64)\n",
      "tensor(7.7956, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(imported['pAct'])\n",
    "print(t.mean(),\"+-\",t.std())\n",
    "threshold = t.mean()+t.std()\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2979bf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[936, 34], edge_index=[2, 1998], edge_attr=[1998, 9], y=[32], batch=[936], ptr=[33])\n",
      "x: torch.Size([936, 34]) edge_index: torch.Size([2, 1998]) y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# ---------- usage ----------\n",
    "# Suppose you already have a DataFrame `df` like in your screenshot.\n",
    "dataset_root = \"./data\"   # choose where to store processed files\n",
    "dataset = build_chembl_dataset_from_df(imported, dataset_root, threshold=threshold)\n",
    "\n",
    "# Dataloaders (graph classification)\n",
    "loaders = get_dataloader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    stratified=False,                 # IMPORTANT: labels are scalar 0/1; don't use the one-hot stratified branch\n",
    "    random_split_flag=True,\n",
    "    data_split_ratio=[0.8, 0.1, 0.1],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Peek at one training batch\n",
    "batch = next(iter(loaders['train']))\n",
    "print(batch)\n",
    "print(\"x:\", batch.x.shape, \"edge_index:\", batch.edge_index.shape, \"y:\", batch.y.shape)\n",
    "\n",
    "# Now you can pass `loaders` + `dataset` to your existing training code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "198f7552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5343f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 32\n",
      "feat_dim: 34\n",
      "edges: 2030 undirected? True\n",
      "edge_attr tensor? True\n",
      "edge_attr shape: torch.Size([2030, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import is_undirected\n",
    "\n",
    "b = next(iter(loaders['train']))\n",
    "print(\"batch size:\", b.y.numel())\n",
    "print(\"feat_dim:\", b.x.size(1))\n",
    "print(\"edges:\", b.edge_index.size(1), \"undirected?\", is_undirected(b.edge_index))\n",
    "ea = getattr(b, \"edge_attr\", None)\n",
    "print(\"edge_attr tensor?\", isinstance(ea, torch.Tensor))\n",
    "if isinstance(ea, torch.Tensor):\n",
    "    print(\"edge_attr shape:\", ea.shape)  # should be [num_directed_edges, 9]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphcw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
