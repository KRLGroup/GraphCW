{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michelaproietti/.conda/envs/thesis/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from omegaconf import OmegaConf\n",
    "from utils import check_dir\n",
    "from gnnNets import *\n",
    "from dataset import get_dataset, get_dataloader\n",
    "from plot_functions import concept_gradient_importance\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch_geometric.nn.models.explainer import clear_masks, set_masks\n",
    "import logging\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "import captum.attr._utils.common\n",
    "\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dig.xgraph.method import GNNExplainer, DeepLIFT\n",
    "from dig.xgraph.evaluation import XCollector\n",
    "from dig.xgraph.method.subgraphx import PlotUtils\n",
    "\n",
    "from torch_geometric.nn.models.explainer import (\n",
    "    Explainer,\n",
    "    clear_masks,\n",
    "    set_masks,\n",
    ")\n",
    "\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data\n",
    "from visualize import visualize_attrs\n",
    "from dig.xgraph.dataset.mol_dataset import *\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from PIL import Image\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michelaproietti/.conda/envs/thesis/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:\n",
      "  gnn_saving_dir: ''\n",
      "  gnn_name: gat\n",
      "  n_heads: 3\n",
      "  param:\n",
      "    hiv:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 64\n",
      "      num_epochs: 200\n",
      "      num_early_stop: 20\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: sum\n",
      "      fc_latent_dim:\n",
      "      - 128\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    sider:\n",
      "      learning_rate: 0.0001\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 64\n",
      "      num_epochs: 200\n",
      "      num_early_stop: 30\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: sum\n",
      "      fc_latent_dim:\n",
      "      - 128\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    clintox:\n",
      "      learning_rate: 0.0005\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 64\n",
      "      num_epochs: 200\n",
      "      num_early_stop: 5\n",
      "      gnn_latent_dim:\n",
      "      - 64\n",
      "      - 64\n",
      "      - 64\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: sigmoid\n",
      "      readout: sum\n",
      "      fc_latent_dim:\n",
      "      - 128\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: elu\n",
      "    tox21:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 64\n",
      "      num_epochs: 200\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: sigmoid\n",
      "      readout: sum\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: elu\n",
      "    ba_lrp:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 32\n",
      "      num_epochs: 200\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: max\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    bbbp:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 64\n",
      "      num_epochs: 50\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: sum\n",
      "      fc_latent_dim:\n",
      "      - 128\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    graph_sst2:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 128\n",
      "      num_epochs: 50\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: max\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    graph_sst5:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 128\n",
      "      num_epochs: 50\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: max\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    twitter:\n",
      "      learning_rate: 0.001\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 128\n",
      "      num_epochs: 50\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: true\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: max\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    ba_shapes:\n",
      "      learning_rate: 0.05\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 1\n",
      "      num_epochs: 400\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 20\n",
      "      - 20\n",
      "      - 20\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: false\n",
      "      node_classification: true\n",
      "      gnn_nonlinear: relu\n",
      "      readout: identity\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    ba_community:\n",
      "      learning_rate: 0.01\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 1\n",
      "      num_epochs: 800\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 20\n",
      "      - 20\n",
      "      - 20\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: true\n",
      "      graph_classification: false\n",
      "      node_classification: true\n",
      "      gnn_nonlinear: relu\n",
      "      readout: identity\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    tree_grid:\n",
      "      learning_rate: 0.01\n",
      "      weight_decay: 0.0005\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 1\n",
      "      num_epochs: 800\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 40\n",
      "      - 40\n",
      "      - 40\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: true\n",
      "      graph_classification: false\n",
      "      node_classification: true\n",
      "      gnn_nonlinear: relu\n",
      "      readout: identity\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    tree_cycle:\n",
      "      learning_rate: 0.01\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 1\n",
      "      num_epochs: 800\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 20\n",
      "      - 20\n",
      "      - 20\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: true\n",
      "      graph_classification: false\n",
      "      node_classification: true\n",
      "      gnn_nonlinear: relu\n",
      "      readout: identity\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    ba_2motifs:\n",
      "      learning_rate: 0.01\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 32\n",
      "      num_epochs: 300\n",
      "      num_early_stop: 0\n",
      "      gnn_latent_dim:\n",
      "      - 20\n",
      "      - 20\n",
      "      - 20\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: mean\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "    bace:\n",
      "      learning_rate: 0.01\n",
      "      weight_decay: 0.0\n",
      "      milestones: None\n",
      "      gamma: None\n",
      "      batch_size: 128\n",
      "      num_epochs: 400\n",
      "      num_early_stop: 20\n",
      "      gnn_latent_dim:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      gnn_dropout: 0.0\n",
      "      add_self_loop: true\n",
      "      gcn_adj_normalization: false\n",
      "      gnn_emb_normalization: false\n",
      "      graph_classification: true\n",
      "      node_classification: false\n",
      "      gnn_nonlinear: relu\n",
      "      readout: max\n",
      "      fc_latent_dim: []\n",
      "      fc_dropout: 0.0\n",
      "      fc_nonlinear: relu\n",
      "datasets:\n",
      "  dataset_root: ~/thesis/datasets\n",
      "  dataset_name: bbbp\n",
      "  random_split_flag: true\n",
      "  data_split_ratio:\n",
      "  - 0.8\n",
      "  - 0.1\n",
      "  - 0.1\n",
      "  seed: 123\n",
      "explainers:\n",
      "  explanation_result_dir: ''\n",
      "  sparsity: 0.5\n",
      "  param:\n",
      "    bbbp:\n",
      "      none: None\n",
      "    graph_sst2:\n",
      "      none: None\n",
      "    graph_sst5:\n",
      "      none: None\n",
      "    twitter:\n",
      "      none: None\n",
      "    ba_shapes:\n",
      "      none: None\n",
      "    ba_community:\n",
      "      none: None\n",
      "    tree_grid:\n",
      "      none: None\n",
      "    tree_cycle:\n",
      "      none: None\n",
      "    ba_2motifs:\n",
      "      none: None\n",
      "    bace:\n",
      "      none: None\n",
      "device_id: 0\n",
      "model_name: GCN_3l\n",
      "record_filename: none\n",
      "statistics_dir: /home/michelaproietti/thesis_last/trainings/bbbp/\n",
      "statistics_file: /home/michelaproietti/thesis_last/trainings/bbbp/gin_newbaseline.json\n",
      "stratified: false\n",
      "train_flag: true\n",
      "concept_whitening: true\n",
      "concept_dir: /home/michelaproietti/thesis_last/molecular_concepts/\n",
      "concepts: qed,mol_weight,hba,logp,n_heteroatoms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"config\", job_name=\"test_app\")\n",
    "cfg = compose(config_name=\"config\", overrides=[])\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.models.gnn_saving_dir = 'gnn_checkpoints'\n",
    "config.models.param = config.models.param[config.datasets.dataset_name]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', index=config.device_id)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "dataset = get_dataset(dataset_root=config.datasets.dataset_root,\n",
    "                      dataset_name=config.datasets.dataset_name)\n",
    "dataset.data.x = dataset.data.x.float()\n",
    "dataset.data.y = dataset.data.y.squeeze().long()\n",
    "if config.models.param.graph_classification:\n",
    "    dataloader_params = {'batch_size': 1,\n",
    "                         'stratified': config.stratified,\n",
    "                         'random_split_flag': config.datasets.random_split_flag,\n",
    "                         'data_split_ratio': config.datasets.data_split_ratio,\n",
    "                         'seed': config.datasets.seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset_root=config.datasets.dataset_root,\n",
    "                          dataset_name=config.datasets.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(dataset, **dataloader_params)\n",
    "test_indices = dataloader['test'].dataset.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_gnnNets(dataset.num_node_features, dataset.num_classes, config.models, config.concept_whitening, concept_acts=False)\n",
    "if config.concept_whitening:\n",
    "    model.replace_norm_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNet(\n",
       "  (readout_layer): GNNPool()\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(9, 128, heads=3)\n",
       "    (1): GATConv(384, 128, heads=3)\n",
       "    (2): GATConv(384, 128, heads=3)\n",
       "  )\n",
       "  (norm_layers): ModuleList(\n",
       "    (0): IterNormRotation(\n",
       "      384, num_channels=384, T=5, eps=1e-05, momentum=0.2, affine=False\n",
       "      (topkpool): TopKPooling(384, ratio=0.5, multiplier=1.0)\n",
       "    )\n",
       "    (1): IterNormRotation(\n",
       "      384, num_channels=384, T=5, eps=1e-05, momentum=0.2, affine=False\n",
       "      (topkpool): TopKPooling(384, ratio=0.5, multiplier=1.0)\n",
       "    )\n",
       "    (2): IterNormRotation(\n",
       "      384, num_channels=384, T=5, eps=1e-05, momentum=0.2, affine=False\n",
       "      (topkpool): TopKPooling(384, ratio=0.5, multiplier=1.0)\n",
       "    )\n",
       "  )\n",
       "  (mlps): ModuleList(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved = torch.load(f\"/home/michelaproietti/thesis_last/trained_models/bbbp/{config.models.gnn_name}_cw_max.pth\")\n",
    "state_dict = saved['net']\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_smiles(smiles, y):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x = []\n",
    "        x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
    "        x.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
    "        x.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
    "        x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
    "        x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
    "        x.append(x_map['num_radical_electrons'].index(\n",
    "            atom.GetNumRadicalElectrons()))\n",
    "        x.append(x_map['hybridization'].index(\n",
    "            str(atom.GetHybridization())))\n",
    "        x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
    "        x.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
    "        xs.append(x)\n",
    "\n",
    "    x = torch.tensor(xs, dtype=torch.long).view(-1, 9)\n",
    "\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = []\n",
    "        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
    "        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
    "        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    # Sort indices.\n",
    "    if edge_index.numel() > 0:\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y,\n",
    "                smiles=smiles, mol=mol)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_collector = XCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/205 [00:00<?, ?it/s]/home/michelaproietti/thesis_last/iterative_normalization.py:82: UserWarning: This overload of baddbmm is deprecated:\n",
      "\tbaddbmm(Number beta, Tensor input, Number alpha, Tensor batch1, Tensor batch2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tbaddbmm(Tensor input, Tensor batch1, Tensor batch2, *, Number beta, Number alpha, Tensor out) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  Sigma = torch.baddbmm(eps, P[0], 1. / m, xc, xc.transpose(1, 2)) # In the paper: 1/n *(Z-mu*1^T)(Z-mu*1^T)^T\n",
      " 10%|▉         | 20/205 [01:58<16:29,  5.35s/it]RDKit WARNING: [14:57:18] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:57:18] WARNING: not removing hydrogen atom without neighbors\n",
      " 31%|███       | 63/205 [05:48<12:39,  5.35s/it]RDKit WARNING: [15:01:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:01:08] WARNING: not removing hydrogen atom without neighbors\n",
      " 37%|███▋      | 76/205 [06:57<11:26,  5.32s/it]RDKit WARNING: [15:02:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:02:17] WARNING: not removing hydrogen atom without neighbors\n",
      " 50%|█████     | 103/205 [09:21<09:07,  5.37s/it]RDKit WARNING: [15:04:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:04:41] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 205/205 [18:28<00:00,  5.41s/it]\n"
     ]
    }
   ],
   "source": [
    "dst = '/home/michelaproietti/thesis_last/fidelity/'\n",
    "dst_explanations = dst + 'explanations/'\n",
    "if not os.path.exists(dst):\n",
    "    os.mkdir(dst)\n",
    "if not os.path.exists(dst_explanations):\n",
    "    os.mkdir(dst_explanations)\n",
    "    \n",
    "explainer_type = 'gnnexplainer' # Other explainer types can be implemented\n",
    "CW = config.concept_whitening\n",
    "plus_fidelity_scores, minus_fidelity_scores = [], []\n",
    "\n",
    "for index,data in enumerate(tqdm(dataset[test_indices])):\n",
    "    data.x = data.x.float()\n",
    "    \n",
    "    data_new = data_from_smiles(data.smiles, data.y)\n",
    "    data_new.x = data_new.x.float()\n",
    "    data_new.to(device)\n",
    "\n",
    "    clear_masks(model)\n",
    "    pred = model(data_new)\n",
    "    pred = torch.softmax(pred, dim=-1)[0,0].item()\n",
    "    \n",
    "    if explainer_type == 'gnnexplainer':\n",
    "        gnn_explainer = GNNExplainer(model, epochs=100, lr=0.01, explain_graph=True)\n",
    "        gnn_explainer.device = device\n",
    "        gnn_explainer._to_log_prob = lambda x: x[0]\n",
    "\n",
    "        clear_masks(model)\n",
    "\n",
    "        edge_mask, hard_edge_mask, related_preds = \\\n",
    "                        gnn_explainer(data_new.x, data_new.edge_index,\n",
    "                                      sparsity=0.8,\n",
    "                                      num_classes=dataset.num_classes)\n",
    "\n",
    "    minus_edge_mask = edge_mask[0]\n",
    "    plus_edge_mask = torch.ones(minus_edge_mask.shape).to(device) - minus_edge_mask\n",
    "\n",
    "    set_masks(model,  torch.nn.Parameter(plus_edge_mask.to(device)), data_new.edge_index, apply_sigmoid=False)\n",
    "    pred_plus = model(data_new)\n",
    "    pred_plus = torch.softmax(pred_plus, dim=-1)[0,0].item()\n",
    "    plus_fidelity_scores.append(pred - pred_plus)\n",
    "    \n",
    "    clear_masks(model)\n",
    "    \n",
    "    set_masks(model,  torch.nn.Parameter(minus_edge_mask.to(device)), data_new.edge_index, apply_sigmoid=False)\n",
    "    pred_minus = model(data_new)\n",
    "    pred_minus = torch.softmax(pred_minus, dim=-1)[0,0].item()\n",
    "    minus_fidelity_scores.append(pred - pred_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity+: -0.2630177048926458 pm 0.5601574488888476\n",
      "Fidelity-: 0.23757027873328637 pm 0.3820786015093287\n"
     ]
    }
   ],
   "source": [
    "print(f'Fidelity+: {np.mean(plus_fidelity_scores)} pm {np.std(plus_fidelity_scores)}')\n",
    "print(f'Fidelity-: {np.mean(minus_fidelity_scores)} pm {np.std(minus_fidelity_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
